{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1a940-ea21-4729-8250-1d753c309cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can run this in your env if you dont have autotime\n",
    "#!conda install -c conda-forge jupyterlab_execute_time\n",
    "%load_ext autotime    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c0b2a-7322-4442-8a5f-8aeda08633cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594ca9d-2f72-42bf-adf9-c17988e5a394",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PART 1: AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24d9c6-3944-4283-b423-6353847652b9",
   "metadata": {},
   "source": [
    "### What is a torch tensor?\n",
    " - alot like a numpy array\n",
    " - desinged for high performance\n",
    " - especially designed to calculate partical derivatives\n",
    " - the pytorch neural network module is meant to work with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eeb1ce-4a83-44c9-b7b4-ea1933850f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple start\n",
    "X = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2abb1-997a-49cd-8dc6-be94f8a4bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e81b30-6c9e-4878-8727-a3af64456e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5250e67-82a3-4c10-ba7b-81328ae7295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3353b07-ddcd-4428-bc71-8ec0f9bca416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132b386-1fbc-4bcf-89c8-5694638e0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94205123-9813-40fc-8348-337867bb4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773106c3-79a1-457b-9449-d79d4d96ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "np_x = np.linspace(-3,3,T) \n",
    "np_t = np.linspace(0,2,T) \n",
    "\n",
    "tr_x = torch.linspace(-3,3,T, requires_grad=True)\n",
    "tr_t = torch.linspace(0,2,T, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e7bbc-48e7-4764-9919-ce5ea6fb2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x**3 / 10\n",
    "\n",
    "np_y = cube(np_x)\n",
    "tr_y = cube(tr_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832af16-9bc1-432c-ab45-7212b45b2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36c6e2-fec6-425e-8cdc-360f77f239b9",
   "metadata": {},
   "source": [
    "### note: grad not stored in y, but x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07024e4-ca89-4af0-a848-cac60b7b48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3735e1-227d-4280-a785-ae918781e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "#ax[0].plot(tr_x, tr_x.grad)\n",
    "ax[0].plot(tr_x.data, tr_x.grad)\n",
    "\n",
    "ax[1].plot(np_x[1:], np.diff(np_y)/(np_x[1]-np_x[0]))\n",
    "\n",
    "titles=['AUTOGRAD','NUMERIC']\n",
    "for a,title in zip(ax.ravel(),titles):\n",
    "    a.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9631a-4778-4d26-b3a3-01ff0976c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_x.grad = None\n",
    "\n",
    "def multivar(x,t):\n",
    "    assert type(x) == type(t), 'types must match'\n",
    "    assert type(x) in [np.ndarray,torch.Tensor], 'type must be torch Tensor or np array'\n",
    "    \n",
    "    if type(x) == np.ndarray:\n",
    "        return np.sin(3*x)/3 + np.log(t+.1)\n",
    "    if type(x) == torch.Tensor:\n",
    "        return torch.sin(3*tr_x)/3 + torch.log(tr_t+.1)\n",
    "    \n",
    "np_y = multivar(np_x, np_t)\n",
    "tr_y = multivar(tr_x, tr_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3146d952-9aba-44f7-ae96-b12a5fe1d887",
   "metadata": {},
   "source": [
    "## Calculating the partials is really easy with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e1776-53e5-40bc-9dc5-a7d9ae5aac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835165b2-3a76-4274-b50e-9ce554ac122a",
   "metadata": {},
   "source": [
    "## less so using numerical derivative\n",
    "\n",
    "### and, of course, the numerical one will have discretization errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a1213-5842-40b6-bffb-6a2719fb36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f(x+dx, t) - f(x, t)\n",
    "dy_x = multivar(np_x[1:], np_t[1:]) - multivar(np_x[:-1], np_t[1:])\n",
    "\n",
    "# f(x,t+dt) - f(x, t)\n",
    "dy_t = multivar(np_x[:-1], np_t[1:]) - multivar(np_x[:-1], np_t[:-1])\n",
    "\n",
    "dx = np_x[1]-np_x[0]\n",
    "dt = np_t[1]-np_t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fba83-f7a8-451d-827e-79f134277bfe",
   "metadata": {},
   "source": [
    "## Let's plot our results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32614aa-a48f-4138-b95b-6b728cd354f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(8,8), sharey='col')\n",
    "ax[0,0].plot(tr_x.detach(), tr_x.grad)\n",
    "ax[0,1].plot(tr_t.detach(), tr_t.grad)\n",
    "\n",
    "ax[1,0].plot(np_x[:-1], dy_x/dx)\n",
    "ax[1,1].plot(np_t[:-1], dy_t/dt)\n",
    "\n",
    "titles=['x.grad','t.grad','numeric x','numeric t']\n",
    "for a,title in zip(ax.ravel(),titles):\n",
    "    a.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af79c7-2978-4fc2-b7ca-d23ed763ed4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Part 2: nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe852da1-01e9-4a30-9960-fc79e801e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d14ac1-265c-4fe2-b180-cdd06dd31ebf",
   "metadata": {},
   "source": [
    "### The crucial pieces of a neural network module are\n",
    " - #### forward method\n",
    " - #### nn.Parameters attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae63d2-7d07-4fe7-b3aa-ef715df98171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_params = torch.randn(2)\n",
    "        self.weights = nn.Parameter( torch.tensor( [self.init_params[0],self.init_params[1]] ))\n",
    "    def forward(self, input: torch.tensor) -> torch.tensor:\n",
    "        return (self.weights * input**2).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be3e6c-d576-41bb-aaa0-83c75c99d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "circArea = SimpleModule()\n",
    "test_point = torch.tensor([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a81f4-ab32-486d-8027-6e59d09aeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = circArea.init_params;\n",
    "print(params)\n",
    "\n",
    "#compare the forward method\n",
    "print( circArea(test_point) )\n",
    "\n",
    "#to manual evaluation\n",
    "print( params[0]*test_point[0]**2+params[1]*test_point[1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925edbbd-e637-41c9-bde3-9a5773d02698",
   "metadata": {},
   "source": [
    "#### Let's say we are trying to learn the sum of the squares..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7175a5-e049-484b-b1e2-64bf993fa8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.tensor(np.random.normal(0,1,(10,2)))\n",
    "measured_data = (input_data**2).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a13c2-e05c-4381-82b3-6fb1bab99e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "circArea.train()\n",
    "\n",
    "#L2 norm\n",
    "loss = ( (circArea(input_data)-measured_data)**2).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fd2d6-cd33-4616-b132-55dc137a45bc",
   "metadata": {},
   "source": [
    "### just for clarity about what .backward does, lets take a look at the model parameters before and after\n",
    "- ####  note None for gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882b3cb-97df-4494-807d-525667434c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in circArea.parameters():\n",
    "    print('params:', p,)\n",
    "    print('gradients:',p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547bc6e-4390-4482-9421-d7a4751add66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547dea33-260e-4113-bef1-d80a31c12968",
   "metadata": {},
   "source": [
    "#### after .backward...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63bd145-7f1c-43c0-8225-71e1f24ea867",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in circArea.parameters():\n",
    "    print('params:', p,)\n",
    "    print('gradients:',p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ddcff-5730-4678-8d45-3d771c78fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1E-1\n",
    "with torch.no_grad():\n",
    "    for param in circArea.parameters():\n",
    "        param -= learning_rate * param.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe235a-be2b-4bab-b0cb-0381242c6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in circArea.parameters():\n",
    "    print('params:', p,)\n",
    "    print('gradients:',p.grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b38b6a-b4b1-4607-b1ab-ab3fdccfd870",
   "metadata": {},
   "source": [
    "## Note that .grad is no longer 'None'\n",
    "\n",
    "### we will have to manually reset it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33e5cb-94a1-4023-8098-b685d60ae06a",
   "metadata": {},
   "source": [
    "## Let's try it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639ce15-db3e-4564-bc9a-fbfb9870784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.tensor(np.random.normal(0,1,(10,2)), requires_grad=True)\n",
    "measured_data = (input_data**2).sum(dim=-1)\n",
    "\n",
    "loss = ( (circArea(input_data)-measured_data)**2).mean()\n",
    "\n",
    "#DONT FORGET TO RESET THE GRAD\n",
    "circArea.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c803b-1bf1-4ba5-90a3-06897a139f7e",
   "metadata": {},
   "source": [
    "#### calculate the gradient...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b8702-6172-4bee-a2d0-6e109cb50f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7b812-f12b-4ede-9810-daf88f08884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in circArea.parameters():\n",
    "    print('params:', p,)\n",
    "    print('gradients:',p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1661a-1468-45f1-b0fa-37014aa71e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1E-1\n",
    "with torch.no_grad():\n",
    "    for param in circArea.parameters():\n",
    "        param -= learning_rate * param.grad\n",
    "        \n",
    "circArea.zero_grad()\n",
    "\n",
    "for p in circArea.parameters():\n",
    "    print(p, p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9821c2-8126-4c6c-86e4-9ad571d5a9a9",
   "metadata": {},
   "source": [
    "### Obviously, we should loop this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadb808-b031-4740-b863-3e428c59c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 250\n",
    "n_samples = 10\n",
    "learning_rate = 1E-2\n",
    "\n",
    "# we'll store results in these\n",
    "losses = []\n",
    "p_array = np.empty((n_iter,2))\n",
    "grads = []\n",
    "\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    \n",
    "    p_array[i,:] = circArea.weights.data\n",
    "    \n",
    "    input_data = torch.tensor(np.random.normal(0,1,(n_samples,2)), requires_grad=True)\n",
    "    noise = torch.normal(0,.1,size=(n_samples,1))\n",
    "    measured_data = ( input_data**2 + noise  ).sum(dim=-1)\n",
    "    \n",
    "    circArea.train()\n",
    "    loss = ( (circArea(input_data)-measured_data)**2 ).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        for p in circArea.parameters():\n",
    "            grads.append(p.grad)\n",
    "            p -= learning_rate * p.grad \n",
    "        \n",
    "    circArea.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531c40c-a5c9-4932-8b83-00f3b30b41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "\n",
    "\n",
    "ax[0].plot(losses)\n",
    "\n",
    "ax[1].plot(p_array)\n",
    "\n",
    "ax[2].plot(grads)\n",
    "\n",
    "titles=['LOSS','WEIGHTS','GRAD']\n",
    "for a,title in zip(ax.ravel(),titles):\n",
    "    a.set_title(title)\n",
    "\n",
    "ax[0].set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76294fce-5aa8-4bac-964f-ce56e0d3a7fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PART 3: standard nn Layers and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ef44d-7d87-46fc-a2f7-84133d9d758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93344642-6b28-4c87-87cb-b4fa131aa5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l_out = nn.Linear(5, 1)\n",
    "                               \n",
    "    def forward(self, input: torch.tensor) -> torch.tensor:\n",
    "          return self.l_out(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511ad9a-c97c-4728-82b0-7b93c29f5d95",
   "metadata": {},
   "source": [
    "#### what does this 'linear' layer do?\n",
    "\n",
    " - linear model of the form W $\\cdot$ X + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191af57e-9d5d-4c59-8dab-a7eb3d2e0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = SimpleModule()\n",
    "\n",
    "test_point = torch.tensor([1,2,3,4,5]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd22c4-b213-47fa-8488-23f0d41b92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in myModel.named_parameters():\n",
    "    print(f'{name}:', param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a711ca-3261-4702-82e7-a4b44b602bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = [p.data for p in myModel.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ecd10-1a13-4c21-9762-0063d1a7d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e782c-44cd-43c7-97a5-77e207e9bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('expected outcome value:',torch.matmul(w,test_point)+b)\n",
    "print('model outcome:', myModel(test_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f42b2a-f4a7-40c0-bc66-3cf5216d0a84",
   "metadata": {},
   "source": [
    "## OK, now let's learn something a small linear model like this can learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2be6e0-4a69-4591-a4e0-b3497c03f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def target(data):    \n",
    "    return 4*data[:,1] -  2*data[:,2] + data[:,3] - 3*data[:,4]+ 2\n",
    "\n",
    "def get_data(N):\n",
    "    data = torch.rand(size=(N,5))\n",
    "    \n",
    "    noise = torch.normal(0, .2, size=(1,N))\n",
    "    \n",
    "    labeled_data = target(data)+noise\n",
    "\n",
    "    return data, labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688fec8-07e7-47a2-84d7-63278ad40229",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, figsize=(12,3))\n",
    "x, y = get_data(1000)\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i].scatter(x[:,i], y, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e32a09-89ee-4816-9d92-17e3e166ddd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba39a3-1694-4602-bcad-007b4f24e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "with torch.no_grad():\n",
    "    ax.scatter(y, myModel(x))\n",
    "    ax.plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myModel(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4664e3-b9da-4acf-b4af-93f17e9d8935",
   "metadata": {},
   "source": [
    "## Here we use a more standard pytorch training syntax\n",
    " - use a built in optimizer that does the learning algorithm\n",
    " - use model.train() and model.eval() when woking with pre-built layers\n",
    " - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be37a3-4a52-41df-89f0-4c8c73526085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=0.001)\n",
    "\n",
    "n_iter = 1\n",
    "#n_iter = 2000\n",
    "n_samples = 500\n",
    "\n",
    "losses =[]\n",
    "\n",
    "verbose = True\n",
    "\n",
    "def vprint(*args):\n",
    "    if verbose:\n",
    "        print(*args)\n",
    "\n",
    "myModel.train()\n",
    "\n",
    "for i in range(n_iter):\n",
    "    d_in, d_out = get_data(n_samples)\n",
    "    \n",
    "    loss = ( (d_out-myModel(d_in)[:,0] )**2 ).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    vprint('before optimizer')\n",
    "    for name, param in myModel.named_parameters():\n",
    "        vprint(f'{name}:', param.data)\n",
    "        vprint('grad:'+f'{param.grad}')\n",
    "\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    vprint('after optimizer')\n",
    "    for name, param in myModel.named_parameters():\n",
    "        vprint(f'{name}:', param.data)\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "\n",
    "    myModel.zero_grad()\n",
    "    \n",
    "myModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d05b1-3ecb-404a-a85a-2d9379586b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    ax[0].plot(losses)\n",
    "    ax[1].scatter(y, myModel(x))\n",
    "    ax[1].plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myModel(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccd4ef-6bb5-47be-a9a0-01d131e725cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in myModel.named_parameters():\n",
    "    print(f'{name}:', param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb199c-0393-45e1-9412-e0cc6338e539",
   "metadata": {},
   "source": [
    "## What if we have a more complicated function to learn?\n",
    " - ### still 5 inputs and 1 output, so maybe the same model will still work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162e51d-4623-411f-b6d3-b2d885a8286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = SimpleModule()\n",
    "\n",
    "def target(data):    \n",
    "    return (torch.sign(data[:,0]-.5))* (torch.abs((torch.cos(6*data[:,1]) -  torch.exp( (data[:,2] - data[:,3])/3 ) - 3*data[:,4]**2 + 2 ) )) /2.5\n",
    "\n",
    "def get_data(N):\n",
    "    data = torch.rand(size=(N,5))\n",
    "    \n",
    "    noise = torch.normal(0, .1, size=(1,N))\n",
    "    \n",
    "    labeled_data = target(data) + noise\n",
    "\n",
    "    return data, labeled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded9d1f-da64-4c52-a10d-32aed15d21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, figsize=(12,3))\n",
    "x, y = get_data(1000)\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i].scatter(x[:,i], y, alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1c288-83b8-4876-800d-56e226cd6054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "with torch.no_grad():\n",
    "    ax.scatter(y, myModel(x))\n",
    "    ax.plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myModel(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b07769b-fc5d-4050-94dd-a96ad0af039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=0.01)\n",
    "\n",
    "n_iter = 1000\n",
    "n_samples = 500\n",
    "\n",
    "losses =[]\n",
    "\n",
    "myModel.train()\n",
    "\n",
    "for i in range(n_iter):\n",
    "    d_in, d_out = get_data(n_samples)\n",
    "    \n",
    "    loss = ( (d_out-myModel(d_in)[:,0] )**2 ).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "            \n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    myModel.zero_grad()\n",
    "    \n",
    "myModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd786b0-2d2a-47e1-a855-1ebf9db9c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ax[0].plot(losses)\n",
    "    ax[1].scatter(y, myModel(x))\n",
    "    ax[1].plot(np.linspace(-2,2,10), np.linspace(-2,2,10))\n",
    "    print( ((y-myModel(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6261c1-3eda-4580-946d-247aafcad403",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Part 4: bigger nonlinear models, torch.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c94a7-f0f4-4b9c-a7e2-06d9fc6aada7",
   "metadata": {},
   "source": [
    " -  ## our simple model was not up to the task\n",
    " -  ## lets make a super overkill one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84288286-c4b7-4a81-acfb-0700ff823157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_homogenous_MLP(config):\n",
    "    '''\n",
    "    helper function to make fully connected MLP layers\n",
    "    '''\n",
    "    n_input = config.n_in\n",
    "    n_output= config.n_out\n",
    "    n_hidden = config.layer_size\n",
    "    num_inner = config.n_inner\n",
    "    \n",
    "    linear_layers = [nn.Linear(n_input, n_hidden),\n",
    "                    *[nn.Linear(n_hidden, n_hidden) for i in range(num_inner)],\n",
    "                    nn.Linear(n_hidden, n_output),\n",
    "                    ]\n",
    "    nonlinear_layers = [nn.ReLU(inplace=True) for i in range(len(linear_layers)-1)]\n",
    "    \n",
    "    layers = [None]*(2*len(linear_layers)-1)\n",
    "    \n",
    "    layers[::2] = linear_layers\n",
    "    layers[1::2] = nonlinear_layers\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# takes in a cofig to make an MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.h = get_homogenous_MLP(self.config)\n",
    "                               \n",
    "    def forward(self, input: torch.tensor) -> torch.tensor:\n",
    "          return self.h(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98467b71-8c00-4389-b60f-9de58a2d47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "options = Namespace()\n",
    "\n",
    "options.n_in = 5\n",
    "options.n_out = 1\n",
    "options.n_inner = 2\n",
    "options.layer_size = 128\n",
    "\n",
    "myMLP = SimpleMLP(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c38639-70d4-4579-a68a-c0a5efe83355",
   "metadata": {},
   "outputs": [],
   "source": [
    "myMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c8860-34d6-40b2-ac3b-8c45ed6086d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "with torch.no_grad():\n",
    "    ax.scatter(y, myMLP(x))\n",
    "    ax.plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myMLP(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c1056-1b9a-4365-9ed8-4a5732ea1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(myMLP.parameters(), lr=0.001)\n",
    "\n",
    "n_datasets = 5\n",
    "n_epochs = 150\n",
    "n_samples = 50_000\n",
    "\n",
    "losses =[]\n",
    "\n",
    "myMLP.train()\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    d_in, d_out = get_data(n_samples)\n",
    "\n",
    "    for j in range(n_epochs):\n",
    "        myMLP.zero_grad()\n",
    "            \n",
    "        loss = ( (d_out-myMLP(d_in)[:,0] )**2 ).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.item()) \n",
    "myMLP.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a77dcf-6a1b-4029-bd1c-48a2065aa707",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(losses)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ax[1].scatter(y, myMLP(x))\n",
    "    ax[1].plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myMLP(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf37ce-d617-4eea-b0b3-1d00cbca61d8",
   "metadata": {},
   "source": [
    "### That took like a whole minute!!! Who has that kind of time?!!?\n",
    "\n",
    " - #### now we get to the high performance part of pytorch\n",
    "\n",
    " note: prepare for some pre-planned errors in the following\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff71771-4631-45b1-b736-3982cde98dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f6657-e478-4119-9fc2-2891967e3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "myMLP = SimpleMLP(options)\n",
    "optimizer = torch.optim.Adam(myMLP.parameters(), lr=0.001)\n",
    "\n",
    "losses =[]\n",
    "\n",
    "myMLP.train()\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    d_in, d_out = get_data(n_samples)\n",
    "\n",
    "\n",
    "\n",
    "    # SEND TO MPS\n",
    "    d_in = d_in.to(torch.device('mps'))\n",
    "    # SEND TO MPS\n",
    "\n",
    "    \n",
    "    for j in range(n_epochs):\n",
    "        myMLP.zero_grad()\n",
    "        \n",
    "        loss = ( (d_out-myMLP(d_in)[:,0] )**2 ).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.item())\n",
    "myMLP.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c9024-e57e-4835-8ec4-99da704395b2",
   "metadata": {},
   "source": [
    "### Take a second to see what the problem was..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cea4b-71f4-424f-8f07-6332598fdde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myMLP.to(torch.device('mps'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1ae47-669c-4d85-a5c2-cb0e17b9e095",
   "metadata": {},
   "source": [
    "### model.to(device) sends the model's parameters to the device;\n",
    " - #### model itself does not have a device attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2194b1-6970-412e-9c96-e86add856f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myMLP.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab0794-1356-4164-b433-32e4896ce832",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in myMLP.parameters():\n",
    "    print(p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe2ea4-8b92-45d7-8a93-1a41fafc0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "myMLP = SimpleMLP(options)\n",
    "\n",
    "\n",
    "# SEND TO MPS\n",
    "myMLP.to(torch.device('mps'))\n",
    "# SEND TO MPS\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(myMLP.parameters(), lr=0.001)\n",
    "\n",
    "losses =[]\n",
    "\n",
    "myMLP.train()\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    d_in, d_out = get_data(n_samples)\n",
    "\n",
    "    d_in = d_in.to(torch.device('mps'))\n",
    "    \n",
    "    for j in range(n_epochs):\n",
    "        myMLP.zero_grad()\n",
    "            \n",
    "        loss = ( (d_out-myMLP(d_in)[:,0] )**2 ).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.item())\n",
    "myMLP.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61362988-b892-4f52-9d85-eb97710ea5ef",
   "metadata": {},
   "source": [
    "### Again, take a second to ID the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e195489-b6f5-467a-8fb4-a698f24e8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "myMLP = SimpleMLP(options)\n",
    "myMLP.to(torch.device('mps'))\n",
    "\n",
    "optimizer = torch.optim.Adam(myMLP.parameters(), lr=0.001)\n",
    "\n",
    "losses =[]\n",
    "\n",
    "myMLP.train()\n",
    "\n",
    "for i in range(n_datasets):\n",
    "\n",
    "    # SEND BOTH TO MPS\n",
    "    d_in, d_out = [d.to(torch.device('mps')) for d in  get_data(n_samples) ]\n",
    "    # SEND BOTH TO MPS\n",
    "\n",
    "    \n",
    "    for j in range(n_epochs):\n",
    "        myMLP.zero_grad()\n",
    "            \n",
    "        loss = ( (d_out-myMLP(d_in)[:,0] )**2 ).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.item())   \n",
    "myMLP.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7efc124-9bba-49fe-805d-6cdde2885463",
   "metadata": {},
   "source": [
    "## NOW, all we have to do is copy over the plot function, right?\n",
    "\n",
    "- #### right....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046598f-9b7e-4aba-a183-a1909acafe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(losses)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ax[1].scatter(y, myMLP(x))\n",
    "    ax[1].plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myMLP(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177daf0-872b-4276-b001-7bbb63099759",
   "metadata": {},
   "source": [
    "### easy fix: just send x to the mps backend, right...?\n",
    " - #### right...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592da16d-e4ed-446a-9983-527b9f48253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(losses)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    ax[1].scatter(y, myMLP(x.to(torch.device('mps'))))\n",
    "    ax[1].plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myMLP(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db698b-8be6-4856-abe8-e25c6cb0cf7a",
   "metadata": {},
   "source": [
    "## Takeaway: Keeping Track of Device is very necesarry, or you'll have workarounds along every corner\n",
    "- ### That was not the last error we were gonna have to fix if we kept along that path...\n",
    "- ### Simplest solution is a function, can also be done with a class or a decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f53147-c6d7-4785-b9fe-83ca30da6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(model, get_data_function, training_device, num_datasets, num_epochs, num_samples):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    losses =[]\n",
    "    \n",
    "    model.to(training_device)\n",
    "    model.train()\n",
    "\n",
    "    for i in range(num_datasets):\n",
    "        d_in, d_out = [ item.to(training_device) for item in get_data_function(num_samples)]\n",
    "    \n",
    "        for j in range(num_epochs):            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            loss = ( (d_out-myMLP(d_in)[:,0] )**2 ).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            losses.append(loss.item())\n",
    "     \n",
    "    model.to(torch.device('cpu'))\n",
    "    model.eval()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17923a1-2326-42b2-856f-e0d0c63b2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myMLP = SimpleMLP(options)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "with torch.no_grad():\n",
    "    ax.scatter(y, myMLP(x))\n",
    "    ax.plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myMLP(x)[:,0])**2).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463aaf9-e60f-4ed3-bcdd-3d85c5788b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_mlp(myMLP, get_data, torch.device('mps'), n_datasets, n_epochs, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d16cb-f3d9-4969-a920-acb658750651",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(losses)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ax[1].scatter(y, myMLP(x))\n",
    "    ax[1].plot([y.min(),y.max()], [y.min(),y.max()], c='k')\n",
    "    print( ((y-myMLP(x)[:,0])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f00ba-244e-4c65-b26a-d52fe27445ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
